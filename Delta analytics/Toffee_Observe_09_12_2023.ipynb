{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4bWdX2YYlfoW"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import psycopg2\n",
        "import calendar\n",
        "import random\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gpVNAo1Wljbp"
      },
      "outputs": [],
      "source": [
        "conn = psycopg2.connect(\n",
        "    port = \"5432\",\n",
        "host = \"shopify-merchant-dump.ccm9mnr5avgs.ap-south-1.rds.amazonaws.com\",\n",
        "  user = \"toffee_coffee_roasters_69e3f806d0674953afe3d3fb2f724c7c\",\n",
        "  password = \"toffee_coffee_roasters_69e3f806d0674953afe3d3fb2f724c7c_password\",\n",
        "  database = \"toffee_coffee_roasters_69e3f806d0674953afe3d3fb2f724c7c_db\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XJubchDHlk2V"
      },
      "outputs": [],
      "source": [
        "query1 = \"\"\"select o.id as order_id,\n",
        "            o.created_at::date as order_date,\n",
        "            o.customer_id as user_id,\n",
        "            oli.product_id as product_id,\n",
        "            oli.name as product_name,\n",
        "            CASE\n",
        "        WHEN cancelled_at IS NOT NULL THEN 'Cancelled'\n",
        "         ELSE 'Delivered'\n",
        "     END as order_status,\n",
        "            oli.quantity as quantity,\n",
        "            oli.price as sp\n",
        "            from orders o\n",
        "            join order_line_item oli on oli.order_id = o.id\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wvBJzrC0lm2H"
      },
      "outputs": [],
      "source": [
        "data = pd.read_sql(query1, conn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_wiz3qQDloY0"
      },
      "outputs": [],
      "source": [
        "data[\"order_date\"] = pd.to_datetime(data[\"order_date\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2jKBkdF5lp13",
        "outputId": "0a154a39-d9da-4d89-80a8-936f7223c02a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [order_id, order_date, user_id, product_id, product_name, order_status, quantity, sp]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c0cebcd-914d-4e0d-b2fe-b597aa3236a2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>order_id</th>\n",
              "      <th>order_date</th>\n",
              "      <th>user_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>product_name</th>\n",
              "      <th>order_status</th>\n",
              "      <th>quantity</th>\n",
              "      <th>sp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c0cebcd-914d-4e0d-b2fe-b597aa3236a2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8c0cebcd-914d-4e0d-b2fe-b597aa3236a2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8c0cebcd-914d-4e0d-b2fe-b597aa3236a2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data[data[\"product_id\"] == np.nan]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Dpir6hXQlsZn"
      },
      "outputs": [],
      "source": [
        "data['WeekCount']=data['order_date'].dt.strftime('%Y-w%U')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "muQWWEt6luDC"
      },
      "outputs": [],
      "source": [
        "data = data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3SKxYo6olvb6"
      },
      "outputs": [],
      "source": [
        "data=data.sort_values('order_date')\n",
        "WeekList=data['WeekCount'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HaKW4tBwlxvr"
      },
      "outputs": [],
      "source": [
        "WeekList = WeekList.tolist()\n",
        "output_list = []\n",
        "\n",
        "for i in range(len(WeekList) - 1):\n",
        "    output_list.append(WeekList[i])\n",
        "    current_week = int(WeekList[i].split('-w')[1])\n",
        "    next_week = int(WeekList[i + 1].split('-w')[1])\n",
        "    missing_weeks = next_week - current_week - 1\n",
        "    for j in range(1, missing_weeks + 1):\n",
        "        missing_week = current_week + j\n",
        "        output_list.append(f'{WeekList[i][:5]}w{missing_week:02d}')\n",
        "\n",
        "output_list.append(WeekList[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MCa6FuRDlzfd"
      },
      "outputs": [],
      "source": [
        "orderuserlevel=data.groupby(['order_id','user_id']).agg(\n",
        "          order_date=pd.NamedAgg(column='order_date', aggfunc='min'),\n",
        "\n",
        "    ).reset_index()\n",
        "orderuserlevel=orderuserlevel.sort_values('order_date')\n",
        "orderuserlevel['TrxnRank'] = orderuserlevel.groupby('user_id')['order_date'].rank(method='first')\n",
        "orderuserlevel\n",
        "\n",
        "\n",
        "data=pd.merge(data,\n",
        "         orderuserlevel[['order_id','TrxnRank']],\n",
        "         on=\"order_id\",\n",
        "         how ='left')\n",
        "data['New']=0\n",
        "data['Repeat']=0\n",
        "data.loc[data['TrxnRank']==1,'New']=1\n",
        "data.loc[data['TrxnRank']>1,'Repeat']=1\n",
        "\n",
        "#AOV Contribution\n",
        "data['ordervalue']=data['quantity']*data['sp']\n",
        "data['AOVContribution']=data['ordervalue']/data[\"order_id\"].nunique()\n",
        "data['DeltaDaysinOrders']=(data.groupby('user_id').order_date.shift() - data.order_date).dt.days.abs()\n",
        "data['DeltaOrderValues']=(data.groupby('user_id').ordervalue.shift() - data.ordervalue)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOkKArYrl1CH"
      },
      "outputs": [],
      "source": [
        "weeklyusersegsats = {}\n",
        "for x in output_list:\n",
        "  x3 = data[data['WeekCount'] <= x] #50\n",
        "  try:\n",
        "    userweeklydata = x3.groupby(\"user_id\").agg(\n",
        "          Last_Date=pd.NamedAgg(column='order_date', aggfunc= lambda x: x.nlargest(2).min()),\n",
        "          First_Date=pd.NamedAgg(column='order_date', aggfunc='min'),\n",
        "          OrdersCount=pd.NamedAgg(column='order_id', aggfunc='nunique'),\n",
        "          Order_Value=pd.NamedAgg(column='ordervalue', aggfunc='sum'),\n",
        "          Mean_Days_Delta=pd.NamedAgg(column='DeltaDaysinOrders', aggfunc='mean'),\n",
        "          Mean_OV_Delta=pd.NamedAgg(column='DeltaOrderValues', aggfunc='mean')\n",
        "          ).reset_index()\n",
        "    globalmaxdateweek=x3['order_date'].max()\n",
        "    userweeklydata['Recency']=globalmaxdateweek-userweeklydata['Last_Date']\n",
        "    userweeklydata['Recency']=userweeklydata['Recency'].dt.days.abs()\n",
        "    userweeklydata['Activated_Quater']=userweeklydata['First_Date'].dt.to_period('Q')\n",
        "    userweeklydata['Maturity_Age']=userweeklydata['Last_Date']-userweeklydata['First_Date']\n",
        "    userweeklydata['Maturity_Age']=userweeklydata['Maturity_Age'].dt.days.abs()\n",
        "    userweeklydata['Mean_Days_Delta']=userweeklydata['Mean_Days_Delta'].fillna(0)\n",
        "    userweeklydata['Mean_OV_Delta']=userweeklydata['Mean_OV_Delta'].fillna(0)\n",
        "    userweeklydata['Customer_AOV']=userweeklydata['Order_Value']/userweeklydata['OrdersCount']\n",
        "\n",
        "    r_labels, f_labels,AOV_labels, m_labels,DelAOV_labels = range(4, 0, -1), range(1,5), range(1,5),range(1,5),range(1,5)\n",
        "    userweeklydata['f_score'] = userweeklydata['OrdersCount'].transform(lambda x: pd.qcut(x.rank(method='first'), q = [0, 0.25, 0.5, 0.75, 1], labels = f_labels)).astype('int64')\n",
        "    # userweeklydata['f_score'] = userweeklydata['OrdersCount'].transform(lambda x: pd.qcut(x[~x.duplicated()].rank(method='first'), q=[0, 0.25, 0.5, 0.75, 1], labels=f_labels)).astype('int64')\n",
        "    userweeklydata['r_score'] = userweeklydata['Recency'].transform(lambda x: pd.qcut(x.rank(method='first'), q = [0, 0.25, 0.5, 0.75, 1], labels = r_labels)).astype('int64')\n",
        "    userweeklydata['AOV_score'] = userweeklydata['Customer_AOV'].transform(lambda x: pd.qcut(x.rank(method='first'), q = [0, 0.25, 0.5, 0.75, 1], labels = AOV_labels)).astype('int64')\n",
        "    userweeklydata['m_score'] = userweeklydata['Maturity_Age'].transform(lambda x: pd.qcut(x.rank(method='first'), q = [0, 0.25, 0.5, 0.75, 1], labels = m_labels)).astype('int64')\n",
        "    userweeklydata['DelAOV'] = userweeklydata['Mean_OV_Delta'].transform(lambda x: pd.qcut(x.rank(method='first'), q = [0, 0.25, 0.5, 0.75, 1], labels = DelAOV_labels)).astype('int64')\n",
        "    userweeklydata['SumScore'] = userweeklydata['r_score'] + userweeklydata['m_score'] + userweeklydata['AOV_score']+ userweeklydata['DelAOV'] +userweeklydata['f_score']\n",
        "    def CohortLables(df):\n",
        "        if df['SumScore'] >= 18:\n",
        "            return 'Champions'\n",
        "        elif ((df['m_score'] <=1) and (df['SumScore'] >= 10)and (df['SumScore'] < 18) and (df['r_score'] >1)):\n",
        "            return 'New&Potential'\n",
        "        elif ((df['m_score'] >1) and (df['SumScore'] >= 14)and (df['SumScore'] < 18)and (df['r_score'] >1)):\n",
        "            return 'Mature&Loyal'\n",
        "        elif ((df['m_score'] >1) and (df['SumScore'] >= 10)and (df['SumScore'] < 14)and (df['r_score'] >1)):\n",
        "            return 'Mature&Slow'\n",
        "        elif ((df['m_score'] <=1) and (df['SumScore'] >= 5)and (df['SumScore'] < 10)and (df['r_score'] >1)):\n",
        "            return 'New&Slow'\n",
        "        elif ((df['m_score'] >1) and (df['SumScore'] >= 5)and (df['SumScore'] < 10)and (df['r_score'] >1)):\n",
        "            return 'Mature&ChurnRisk'\n",
        "        else:\n",
        "            return 'Churned'\n",
        "\n",
        "    userweeklydata['CohortLabel'] = userweeklydata.apply(CohortLables, axis=1)\n",
        "\n",
        "    usersegstats=userweeklydata.groupby(\n",
        "      'CohortLabel'\n",
        "    ).agg(\n",
        "          Users=pd.NamedAgg(column='user_id', aggfunc='nunique'),\n",
        "          Orders=pd.NamedAgg(column='OrdersCount', aggfunc='sum'),\n",
        "          Recency=pd.NamedAgg(column='Recency', aggfunc='mean'),\n",
        "          OrderValue=pd.NamedAgg(column='Order_Value', aggfunc='sum'),\n",
        "          MeanDaysDelta=pd.NamedAgg(column='Mean_Days_Delta', aggfunc='mean'),\n",
        "          MeanOVDelta=pd.NamedAgg(column='Mean_OV_Delta', aggfunc='mean'),\n",
        "          Maturity=pd.NamedAgg(column='Maturity_Age', aggfunc='mean'),\n",
        "    )\n",
        "\n",
        "    usersegstats[\"Frequency\"] = usersegstats[\"Orders\"] / usersegstats[\"Users\"]\n",
        "    usersegstats[\"AOVContribution\"] = usersegstats[\"OrderValue\"] / usersegstats[\"Orders\"].sum()\n",
        "    usersegstats[\"AOV\"] = usersegstats[\"OrderValue\"] / usersegstats[\"Orders\"]\n",
        "    user_cohort_labels = ['Champions', 'Churned', 'Mature&ChurnRisk', 'Mature&Loyal', 'Mature&Slow', 'New&Potential', 'New&Slow']\n",
        "    var_list = ['Users', 'Orders', 'Recency', 'AOVContribution', 'OrderValue',\n",
        "        'MeanDaysDelta', 'MeanOVDelta', 'Maturity', 'Frequency', 'AOV']\n",
        "    dummy_df = pd.DataFrame(index=user_cohort_labels, columns=var_list)\n",
        "    for i in user_cohort_labels:\n",
        "        try:\n",
        "          dummy_df.loc[i] = usersegstats.loc[i]\n",
        "        except:\n",
        "          pass\n",
        "    dummy_df = dummy_df.fillna(0)\n",
        "\n",
        "      # usersegstats = usersegstats.reset_index().iloc[:, 1:]\n",
        "    usersegstats = dummy_df\n",
        "    usersegstats = usersegstats.stack()\n",
        "    usersegstats=usersegstats.reset_index()\n",
        "    usersegstats = usersegstats.rename(columns={'level_0': 'User Cohort Lables'})\n",
        "    usersegstats=usersegstats.rename(columns={\"level_1\": \"Product Variable\"})\n",
        "    usersegstats=usersegstats.rename(columns={0: \"Product Value\"})\n",
        "    weeklyusersegsats[x] = usersegstats\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v02q-Az5l2b3"
      },
      "outputs": [],
      "source": [
        "List=output_list\n",
        "productsegstats_weekly = {}\n",
        "\n",
        "\n",
        "for x in List:\n",
        "  x3 = data[data[\"WeekCount\"] <= x]\n",
        "  totalcarts=x3['order_id'].nunique()\n",
        "  productlevel=x3.groupby(['product_id','product_name']).agg(\n",
        "            Carts=pd.NamedAgg(column='order_id', aggfunc='nunique'),\n",
        "            quantity=pd.NamedAgg(column='quantity', aggfunc='sum'),\n",
        "            ASP=pd.NamedAgg(column='sp', aggfunc='mean'),\n",
        "            NewOrders=pd.NamedAgg(column='New', aggfunc='sum'),\n",
        "            RepeatOrders=pd.NamedAgg(column='Repeat', aggfunc='sum'),\n",
        "            Users=pd.NamedAgg(column='user_id', aggfunc='nunique'),\n",
        "            AOVContribution=pd.NamedAgg(column='AOVContribution', aggfunc='sum'),\n",
        "      ).reset_index()\n",
        "  productlevel['CartPenetration']=productlevel['Carts']/totalcarts\n",
        "  productlevel['NewUserRatio']=productlevel['NewOrders']/productlevel['Users']\n",
        "  productlevel['QpC']=productlevel['quantity']/productlevel['Carts']\n",
        "  productlevel['ASPXQpC']=productlevel['ASP']*productlevel['QpC']\n",
        "\n",
        "  CP_labels, NewRatio_labels,ASP_labels, QpC_labels =  range(1,5), range(1,5),range(1,5),range(1,5)\n",
        "\n",
        "  productlevel['CP_score'] = productlevel['CartPenetration'].transform(lambda x: pd.qcut(x.rank(method='first'), q = [0, 0.25, 0.5, 0.75, 1], labels = CP_labels)).astype('int64')\n",
        "  productlevel['NewRatio_score'] = productlevel['NewUserRatio'].transform(lambda x: pd.qcut(x.rank(method='first'), q = [0, 0.25, 0.5, 0.75, 1], labels = NewRatio_labels)).astype('int64')\n",
        "  productlevel['ASP_score'] = productlevel['ASP'].transform(lambda x: pd.qcut(x.rank(method='first'), q = [0, 0.25, 0.5, 0.75, 1], labels = ASP_labels)).astype('int64')\n",
        "  productlevel['QpC_score'] = productlevel['QpC'].transform(lambda x: pd.qcut(x.rank(method='first'), q = [0, 0.25, 0.5, 0.75, 1], labels = QpC_labels)).astype('int64')\n",
        "  productlevel['SumScore'] = productlevel['CP_score'] + productlevel['NewRatio_score'] + productlevel['ASP_score'] + productlevel['QpC_score']\n",
        "\n",
        "  def ProductCohortLables(df):\n",
        "      if df['SumScore'] > 15:\n",
        "          return 'Hero Products'\n",
        "      elif ((df['CP_score'] >3) and (df['SumScore'] >= 8)and (df['SumScore'] <= 15) and (df['NewRatio_score'] >3)):\n",
        "          return 'Marquee & New Business Drivers'\n",
        "      elif ((df['CP_score'] >3) and (df['SumScore'] >= 8)and (df['SumScore'] <= 15)and (df['NewRatio_score'] <=3)):\n",
        "          return 'Marquee & Repeat Business Drivers'\n",
        "\n",
        "      elif ((df['ASP_score'] >3) and (df['SumScore'] >= 5)and (df['SumScore'] < 8)and (df['QpC_score'] <=3)):\n",
        "          return 'High Value & Slow Moving'\n",
        "      elif ((df['ASP_score'] <=3) and (df['SumScore'] >= 5)and (df['SumScore'] < 8)and (df['QpC_score'] >3)):\n",
        "          return 'Low Value & Slow Moving'\n",
        "      else:\n",
        "          return 'Lagging Products'\n",
        "\n",
        "  productlevel['Product Cohort Lables'] = productlevel.apply(ProductCohortLables, axis=1)\n",
        "\n",
        "  merged_df = pd.merge(productlevel, data, on=['product_id', 'product_name', 'quantity', 'AOVContribution'], how='left')\n",
        "\n",
        "  productsegstats=merged_df.groupby('Product Cohort Lables').agg(\n",
        "        ProductCount=pd.NamedAgg(column='product_id', aggfunc='nunique'),\n",
        "        Orders=pd.NamedAgg(column='order_id', aggfunc='nunique'),\n",
        "        AOVContribution=pd.NamedAgg(column='AOVContribution', aggfunc='sum'),\n",
        "        Quantity=pd.NamedAgg(column='quantity', aggfunc='sum'),\n",
        "        ASP=pd.NamedAgg(column='sp', aggfunc='mean'),\n",
        "        NewOrders=pd.NamedAgg(column='New', aggfunc='sum'),\n",
        "        RepeatOrders=pd.NamedAgg(column='Repeat', aggfunc='sum'),\n",
        "        Users=pd.NamedAgg(column='user_id', aggfunc='nunique'))\n",
        "  productsegstats[\"QuantityperCart\"] = productsegstats[\"Quantity\"] / productsegstats[\"Orders\"]\n",
        "  productsegstats[\"CartPenetration\"] = productsegstats[\"Orders\"] / productsegstats[\"Orders\"].sum()\n",
        "  productsegstats[\"NewUserRatio\"] = productsegstats[\"NewOrders\"] / productsegstats[\"Users\"].sum()\n",
        "  productsegstats[\"ASPXQpC\"] = productsegstats[\"QuantityperCart\"] * productsegstats[\"ASP\"]\n",
        "  product_cohort_labels = ['Hero Products', 'Marquee & New Business Drivers', 'Marquee & Repeat Business Drivers', 'High Value & Slow Moving', 'Low Value & Slow Moving', 'Lagging Products']\n",
        "  dummy_df = pd.DataFrame(index=product_cohort_labels, columns=productsegstats.columns.tolist())\n",
        "  for i in product_cohort_labels:\n",
        "    try:\n",
        "      dummy_df.loc[i] = productsegstats.loc[i]\n",
        "    except:\n",
        "      pass\n",
        "    dummy_df = dummy_df.fillna(0)\n",
        "  productsegstats = dummy_df\n",
        "  productsegstats = productsegstats.stack()\n",
        "  productsegstats=productsegstats.reset_index()\n",
        "  productsegstats = productsegstats.rename(columns={'level_0': 'Product Cohort Lables'})\n",
        "  productsegstats=productsegstats.rename(columns={\"level_1\": \"Product Variable\"})\n",
        "  productsegstats=productsegstats.rename(columns={0: \"Product Value\"})\n",
        "  productsegstats_weekly[x] = productsegstats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTdBZ1Pll4IG"
      },
      "outputs": [],
      "source": [
        "weekdf = data.groupby('WeekCount').agg(\n",
        "            totalorders=pd.NamedAgg(column='order_id', aggfunc='nunique'),\n",
        "            totalusers=pd.NamedAgg(column='user_id', aggfunc='nunique'),\n",
        "            #Users=pd.NamedAgg(column='user_id', aggfunc='nunique'),\n",
        "            totalvalue=pd.NamedAgg(column='ordervalue', aggfunc='sum'),\n",
        "            quantity=pd.NamedAgg(column='quantity', aggfunc='sum'),\n",
        "            SKUCounts=pd.NamedAgg(column='product_id', aggfunc='nunique')\n",
        "\n",
        "\n",
        "      ).reset_index()\n",
        "\n",
        "newdf=data[data['New']==1].groupby('WeekCount').agg(\n",
        "    newusercount=pd.NamedAgg(column='user_id', aggfunc='nunique'),\n",
        "     ).reset_index()\n",
        "\n",
        "\n",
        "\n",
        "cancelled_orders = data[data['order_status'] == 'Cancelled'].groupby('WeekCount').agg(\n",
        "    CancellationCount=pd.NamedAgg(column='order_id', aggfunc='count')\n",
        ").fillna(0).reset_index()\n",
        "\n",
        "delivered_orders = data[data['order_status'] == 'Delivered'].groupby('WeekCount').agg(\n",
        "    DeliveryCount=pd.NamedAgg(column='order_id', aggfunc='count')\n",
        ").fillna(0).reset_index()\n",
        "\n",
        "\n",
        "weekdf=weekdf.merge(newdf, on=\"WeekCount\", how=\"left\")\n",
        "weekdf=weekdf.merge(cancelled_orders, on=\"WeekCount\", how=\"left\")\n",
        "weekdf=weekdf.merge(delivered_orders, on=\"WeekCount\", how=\"left\")\n",
        "weekdf['repeatusercount']=weekdf['totalusers']-weekdf['newusercount']\n",
        "weekdf['AOV'] = weekdf['totalvalue']/weekdf['totalorders']\n",
        "weekdf['orderperuser'] = weekdf['totalorders']/weekdf['totalusers']\n",
        "weekdf['ASP'] = weekdf['totalvalue']/weekdf['totalusers']\n",
        "weekdf['quantitypercart'] = weekdf['quantity']/weekdf['totalorders']\n",
        "weekdf['newuserratio']= weekdf['newusercount']/weekdf['totalusers']\n",
        "weekdf['repeatuserratio']=  weekdf['repeatusercount']/weekdf['totalusers']\n",
        "weekdf['SKUperOrder']=  weekdf['SKUCounts']/weekdf['totalorders']\n",
        "weekdf['cancellationrate'] = weekdf['CancellationCount']/weekdf['totalorders']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECHFJcp1l8An"
      },
      "outputs": [],
      "source": [
        "weekdf[\"Rolling_Churned_User_Count\"] = (weekdf.repeatusercount.shift() - weekdf.repeatusercount)\n",
        "weekdf[\"Acquisition_Rate\"] = 0\n",
        "weekdf[\"Repeat_Rate\"] = 0\n",
        "weekdf[\"Churn_Rate\"] = 0\n",
        "for i in range (1, len(weekdf)):\n",
        "  weekdf.loc[i, \"Acquisition_Rate\"] = (weekdf['newusercount'][i] - weekdf['newusercount'][i - 1]) / weekdf['totalusers'][i - 1]\n",
        "  weekdf.loc[i, \"Repeat_Rate\"] = (weekdf['repeatusercount'][i] - weekdf['repeatusercount'][i - 1]) / weekdf['totalusers'][i - 1]\n",
        "  weekdf.loc[i, \"Churn_Rate\"] = weekdf['Rolling_Churned_User_Count'][i] / weekdf['totalusers'][i - 1]\n",
        "weekdf[\"Growth_Rate\"] = weekdf[\"Acquisition_Rate\"] + weekdf[\"Churn_Rate\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-kYquWGl9bV"
      },
      "outputs": [],
      "source": [
        "# weekdf\n",
        "dummy = weekdf[\"WeekCount\"].tolist()\n",
        "if dummy != output_list:\n",
        "  to_add = [x for x in output_list if x not in dummy]\n",
        "  for week_to_add in to_add:\n",
        "    col_to_add = [week_to_add] + ([0] * (len(weekdf.columns.tolist())-1))\n",
        "    weekdf.loc[len(weekdf)] = col_to_add\n",
        "  weekdf = weekdf.sort_values(by = \"WeekCount\", ascending = True)\n",
        "weekdf = weekdf.fillna(0)\n",
        "weekdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxfDxGj0l-5A"
      },
      "outputs": [],
      "source": [
        "data[\"Retention\"] = 0\n",
        "data.loc[(data[\"DeltaDaysinOrders\"] > 0) & (data[\"DeltaDaysinOrders\"] <= 90) , \"Retention\"] = \"Retained\"\n",
        "data.loc[(data[\"DeltaDaysinOrders\"] > 90), \"Retention\"] = \"Not-Retained\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nasrode0mBSP"
      },
      "outputs": [],
      "source": [
        "retained_df = data[data['Retention'] == 'Retained'].groupby('WeekCount').size().reset_index(name='Retained_Users')\n",
        "weekdf = pd.merge(weekdf, retained_df, on='WeekCount')\n",
        "weekdf['Retention_Rate'] = (weekdf['Retained_Users'] / weekdf['totalusers'])\n",
        "weekdf.drop(columns=['Retained_Users'], inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cow2nJpGmC0W"
      },
      "outputs": [],
      "source": [
        "for key, df in productsegstats_weekly.items():\n",
        "    df['CohortLabel_Product_Variable'] = df['Product Cohort Lables'].astype(str) + '_' + df['Product Variable'].astype(str)\n",
        "\n",
        "    productsegstats_weekly[key] = df[['CohortLabel_Product_Variable', 'Product Value']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCt4ZiE6mERe"
      },
      "outputs": [],
      "source": [
        "for key, df in weeklyusersegsats.items():\n",
        "    df['CohortLabel_Product_Variable'] = df['User Cohort Lables'].astype(str) + '_' + df['Product Variable'].astype(str)\n",
        "\n",
        "    weeklyusersegsats[key] = df[['CohortLabel_Product_Variable', 'Product Value']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSHqKoPQmFwA"
      },
      "outputs": [],
      "source": [
        "lstproseg =list(productsegstats_weekly.keys())\n",
        "lstproseg1=lstproseg[0]\n",
        "profinlst = productsegstats_weekly[lstproseg1]['CohortLabel_Product_Variable'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRWWujUOmILl"
      },
      "outputs": [],
      "source": [
        "lstcusseg =list(weeklyusersegsats.keys())\n",
        "lstcusseg1=lstcusseg[0]\n",
        "cusfinlst = weeklyusersegsats[lstcusseg1]['CohortLabel_Product_Variable'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcYm4GcMmKQo"
      },
      "outputs": [],
      "source": [
        "weeks = list(productsegstats_weekly.keys())\n",
        "product_df = pd.DataFrame(index=weeks, columns=profinlst)\n",
        "for week, df in productsegstats_weekly.items():\n",
        "    product_df.loc[week, :] = df['Product Value'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBGhODo4mMY3"
      },
      "outputs": [],
      "source": [
        "weeks = list(weeklyusersegsats.keys())\n",
        "customer_df = pd.DataFrame(index=weeks, columns=cusfinlst)\n",
        "for week, df in weeklyusersegsats.items():\n",
        "    customer_df.loc[week, :] = df['Product Value'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DcuY4RImOiT"
      },
      "outputs": [],
      "source": [
        "merged_pro_cus_df = pd.merge(product_df, customer_df, left_index=True, right_index=True)\n",
        "merged_pro_cus_df.index.name = 'WeekCount'\n",
        "weekly = pd.merge(weekdf, merged_pro_cus_df, on=\"WeekCount\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-3cBXpRmRnT"
      },
      "outputs": [],
      "source": [
        "# weekly.iloc[:, 1:]\n",
        "corr_table = weekly.iloc[:, 1:]\n",
        "corr_table = corr_table.astype(\"float\")\n",
        "corr = corr_table.corr()\n",
        "corr = corr.dropna(axis=0, how=\"all\")\n",
        "corr = corr.dropna(axis=1, how=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1zqwWCCmcXI"
      },
      "outputs": [],
      "source": [
        "corr_selection = {}\n",
        "pdt = [\"Hero Products\", \"Marquee & New Business Drivers\", \"Marquee & Repeat Business Drivers\", \"High Value & Slow Moving\", \"Low Value & Slow Moving\", \"Lagging Products\"]\n",
        "cust = [\"Champions\", \"New&Potential\", \"Mature&Loyal\", \"Mature&Slow\", \"New&Slow\", \"Mature&ChurnRisk\", \"Churned\"]\n",
        "col_main = corr.columns.tolist()\n",
        "col = corr.columns.tolist()[:]\n",
        "col_tar = corr.columns.tolist()[:15]\n",
        "col_pdt = [colx for colx in col_main if colx.split(\"_\")[0] in pdt]\n",
        "col_cust = [colx for colx in col_main if colx.split(\"_\")[0] in cust]\n",
        "\n",
        "\n",
        "for i in col:\n",
        "  correlations = corr[i].drop(index=i)\n",
        "  sorted_corr = correlations.sort_values().index.to_list()\n",
        "  corr_pdt = [x for x in sorted_corr if x in col_pdt if x != i]\n",
        "  corr_pdt_20 = corr_pdt[:10] + corr_pdt[-10:]\n",
        "  corr_cust = [x for x in sorted_corr if x in col_cust if x != i]\n",
        "  corr_cust_20 = corr_cust[:10] + corr_cust[-10:]\n",
        "  corr_tar = [x for x in sorted_corr if x in col_tar if x != i]\n",
        "  corr_tar_14 = corr_tar[:5] + corr_tar[-5:]\n",
        "  corr_selection[i] = corr_pdt_20 + corr_cust_20 + corr_tar_14 + [i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppHm8bwynzat"
      },
      "outputs": [],
      "source": [
        "weekly_pct_change = weekly.iloc[:, 1:].pct_change(axis = 0)\n",
        "weekly_pct_change.index = weekly[\"WeekCount\"].tolist()\n",
        "weekly_pct_change = weekly_pct_change.iloc[:, :15]\n",
        "weekly_pct_change.replace([np.inf, -np.inf], -100, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWkMS8ofpV95"
      },
      "outputs": [],
      "source": [
        "weekly_delta = weekly.iloc[:, 1:].diff(axis = 0)\n",
        "weekly_delta = weekly_delta.iloc[:, :15]\n",
        "weekly_delta.index = weekly[\"WeekCount\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHdOGL6ApbBB"
      },
      "outputs": [],
      "source": [
        "new_df_idx = []\n",
        "week_drop_name = [] #\"2023-w43\"\n",
        "col = weekly_pct_change.columns.tolist()\n",
        "idx = weekly_pct_change.index.tolist()\n",
        "for i in range (1, len(idx)):\n",
        "  for j in range (len(col)):\n",
        "    if idx[i-1] not in week_drop_name and idx[i] not in week_drop_name:\n",
        "      w = f\"{idx[i-1]}_{idx[i]}_{col[j]}\"\n",
        "      new_df_idx.append(w)\n",
        "\n",
        "main = pd.DataFrame(index = new_df_idx)\n",
        "val = []\n",
        "for i in range (1, len(idx)):\n",
        "  for j in range (len(col)):\n",
        "    if idx[i-1] not in week_drop_name and idx[i] not in week_drop_name:\n",
        "      valx = weekly_pct_change.iloc[i, j]\n",
        "      val.append(valx)\n",
        "\n",
        "main[\"Values\"] = val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrQUA_6JBIeb"
      },
      "outputs": [],
      "source": [
        "#Directionality\n",
        "neg_list = [\"CancellationCount\",\n",
        "\"cancellationrate\",\n",
        "\"Champions_Recency\",\n",
        "\"Churned_Recency\",\n",
        "\"Mature&ChurnRisk_Recency\",\n",
        "\"Mature&Loyal_Recency\",\n",
        "\"Mature&Slow_Recency\",\n",
        "\"New&Potential_Recency\",\n",
        "\"New&Slow_Recency\"]\n",
        "\n",
        "for i in range (len(main)):\n",
        "  a = main.index.tolist()[i]\n",
        "  if a.split(\"_\")[2] in neg_list:\n",
        "    main.iloc[i, 0] = -1 * main.iloc[i, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJjpXrXRpprW"
      },
      "outputs": [],
      "source": [
        "main = main.dropna()\n",
        "main = main[main[\"Values\"]!=np.inf]\n",
        "main = main.sort_values(by = \"Values\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nGuIRz3prtG"
      },
      "outputs": [],
      "source": [
        "main1 = main.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxCjT3gipzYy"
      },
      "outputs": [],
      "source": [
        "# Product Page\n",
        "def find_idx (x):\n",
        "  pw = x.split(\"_\")[0]\n",
        "  fw = x.split(\"_\")[1]\n",
        "  v = x.split(\"_\")[2]\n",
        "  zz = \"_\".join([pw, fw])\n",
        "\n",
        "  return str(pw), str(fw), str(v), str(zz)\n",
        "\n",
        "diction = {}\n",
        "\n",
        "for i in range(len(main1)):\n",
        "  pw, fw, v, zz = find_idx(main1.iloc[i, 0])\n",
        "  diction[i] = pw, fw, v, zz\n",
        "new_df = pd.DataFrame.from_dict(diction).T\n",
        "new_df = new_df.rename({0:\"Previous Week\", 1: \"Following Week\", 2: \"Value\", 3:\"Week pair\"}, axis = 1)\n",
        "main1 = pd.merge(main1, new_df, left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVVdAStUN4DC"
      },
      "outputs": [],
      "source": [
        "grouped = main1.groupby('Week pair')\n",
        "week_pair_lst = []\n",
        "best_two_lst = []\n",
        "worst_two_lst = []\n",
        "for name, group in grouped:\n",
        "  sort_week_df = group.sort_values('Values', ascending=False)\n",
        "  sort_week_df_1 = sort_week_df.reset_index().iloc[:,1:]\n",
        "  week_pair = sort_week_df_1.loc[0, 'Week pair']\n",
        "  best_two = sort_week_df_1.loc[[0, 1], 'Value'].tolist()\n",
        "  worst_two = sort_week_df_1.loc[[len(sort_week_df_1) - 1, len(sort_week_df_1) - 2], 'Value'].tolist()\n",
        "  week_pair_lst.append(week_pair)\n",
        "  best_two_lst.append(worst_two)\n",
        "  worst_two_lst.append(best_two)\n",
        "target_df = pd.DataFrame({\"Week pair\": week_pair_lst, \"Best_Two\": best_two_lst, \"Worst_Two\": worst_two_lst})\n",
        "target_df.set_index(\"Week pair\", inplace= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQIzbB9gqyJ8"
      },
      "outputs": [],
      "source": [
        "target_df_new = pd.DataFrame(columns=['Previous Week', 'Current Week', 'Worst 1', 'Worst 2', 'Best 1', 'Best 2'])\n",
        "\n",
        "for index, row in target_df.iterrows():\n",
        "    previous_week, current_week = index.split('_')\n",
        "    worst_values = row['Worst_Two']\n",
        "    best_values = row['Best_Two']\n",
        "\n",
        "    # Check if the array is not empty before accessing its elements\n",
        "    worst_1 = worst_values[0] if len(worst_values) > 0 else None\n",
        "    worst_2 = worst_values[1] if len(worst_values) > 1 else None\n",
        "\n",
        "    best_1 = best_values[0] if len(best_values) > 0 else None\n",
        "    best_2 = best_values[1] if len(best_values) > 1 else None\n",
        "\n",
        "    new_row = {\n",
        "        'Previous Week': previous_week,\n",
        "        'Current Week': current_week,\n",
        "        'Worst 1': worst_1,\n",
        "        'Worst 2': worst_2,\n",
        "        'Best 1': best_1,\n",
        "        'Best 2': best_2\n",
        "    }\n",
        "\n",
        "    target_df_new = target_df_new.append(new_row, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjvtRIzGrSCj"
      },
      "outputs": [],
      "source": [
        "melted_df = pd.melt(target_df_new, id_vars=['Previous Week', 'Current Week'], value_vars=['Worst 1', 'Worst 2', 'Best 1', 'Best 2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kYt1ATEH_9O"
      },
      "outputs": [],
      "source": [
        "distr = melted_df.groupby(\"value\").agg(\n",
        "    Frequency = pd.NamedAgg(column='Current Week', aggfunc='nunique')\n",
        ").reset_index()\n",
        "distr = distr.sort_values(by = 'Frequency')\n",
        "n = int(0.3 * len(distr))\n",
        "\n",
        "unwanted = distr[\"value\"][-n:].tolist()\n",
        "main1_unwanted = main1.copy()\n",
        "main1_wanted = main1.copy()\n",
        "main1_unwanted = main1_unwanted[main1_unwanted[\"Value\"].isin(unwanted)]\n",
        "main1_wanted = main1_wanted[~main1_wanted[\"Value\"].isin(unwanted)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_MuDkXKOS_3"
      },
      "source": [
        "# Wanted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SVYSdQwNqUY"
      },
      "outputs": [],
      "source": [
        "grouped = main1_wanted.groupby('Week pair')\n",
        "week_pair_lst = []\n",
        "best_two_lst = []\n",
        "worst_two_lst = []\n",
        "for name, group in grouped:\n",
        "  sort_week_df = group.sort_values('Values', ascending=False)\n",
        "  sort_week_df_1 = sort_week_df.reset_index().iloc[:,1:]\n",
        "  week_pair = sort_week_df_1.loc[0, 'Week pair']\n",
        "  best_two = sort_week_df_1.loc[[0, 1], 'Value'].tolist()\n",
        "  worst_two = sort_week_df_1.loc[[len(sort_week_df_1) - 1, len(sort_week_df_1) - 2], 'Value'].tolist()\n",
        "  week_pair_lst.append(week_pair)\n",
        "  best_two_lst.append(worst_two)\n",
        "  worst_two_lst.append(best_two)\n",
        "target_df = pd.DataFrame({\"Week pair\": week_pair_lst, \"Best_Two\": best_two_lst, \"Worst_Two\": worst_two_lst})\n",
        "target_df.set_index(\"Week pair\", inplace= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "324jWGvsNqSG"
      },
      "outputs": [],
      "source": [
        "target_df_new = pd.DataFrame(columns=['Previous Week', 'Current Week', 'Worst 1', 'Worst 2', 'Best 1', 'Best 2'])\n",
        "\n",
        "for index, row in target_df.iterrows():\n",
        "    previous_week, current_week = index.split('_')\n",
        "    worst_values = row['Worst_Two']\n",
        "    best_values = row['Best_Two']\n",
        "\n",
        "    # Check if the array is not empty before accessing its elements\n",
        "    worst_1 = worst_values[0] if len(worst_values) > 0 else None\n",
        "    worst_2 = worst_values[1] if len(worst_values) > 1 else None\n",
        "\n",
        "    best_1 = best_values[0] if len(best_values) > 0 else None\n",
        "    best_2 = best_values[1] if len(best_values) > 1 else None\n",
        "\n",
        "    new_row = {\n",
        "        'Previous Week': previous_week,\n",
        "        'Current Week': current_week,\n",
        "        'Worst 1': worst_1,\n",
        "        'Worst 2': worst_2,\n",
        "        'Best 1': best_1,\n",
        "        'Best 2': best_2\n",
        "    }\n",
        "\n",
        "    target_df_new = target_df_new.append(new_row, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeEu8O_ZNqQx"
      },
      "outputs": [],
      "source": [
        "melted_df = pd.melt(target_df_new, id_vars=['Previous Week', 'Current Week'], value_vars=['Worst 1', 'Worst 2', 'Best 1', 'Best 2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFlp3varNqOF"
      },
      "outputs": [],
      "source": [
        "values_dict = {}\n",
        "\n",
        "for index, row in melted_df.iterrows():\n",
        "    previous_week = row['Previous Week']\n",
        "    current_week = row['Current Week']  # Assuming you have a column named 'Current Week'\n",
        "    variable = row['value']\n",
        "\n",
        "    # Use these values to get the corresponding value from the original DataFrame for the previous week\n",
        "    previous_week_value = weekly[(weekly[\"WeekCount\"] == previous_week)][[variable]] #weekly.loc[previous_week, variable]#if pd.notna(previous_week) and variable in weekly.columns else 0\n",
        "    previous_week_value = previous_week_value.values[0][0]\n",
        "\n",
        "    # Use these values to get the corresponding value from the original DataFrame for the current week\n",
        "    current_week_value = weekly[(weekly[\"WeekCount\"] == current_week)][[variable]] #current_week_value = weekly.loc[current_week, variable] #if pd.notna(current_week) and variable in weekly.columns else 0\n",
        "    current_week_value = current_week_value.values[0][0]\n",
        "\n",
        "    # Calculate the difference between Previous Week Value and Current Week Value\n",
        "    difference = current_week_value - previous_week_value\n",
        "\n",
        "    # Add the values to the dictionary with the corresponding index and column names\n",
        "    values_dict[index] = {\n",
        "        'Previous Week': previous_week,\n",
        "        'Current Week': current_week,\n",
        "        'Target Variable': variable,\n",
        "        'Previous Week Value': previous_week_value,\n",
        "        'Current Week Value': current_week_value,\n",
        "        'Difference': difference,\n",
        "        'Tagging': row['variable']  # Assuming 'variable' is the column name in melted_df\n",
        "    }\n",
        "\n",
        "# Create a new DataFrame from the dictionary\n",
        "new_dataframe = pd.DataFrame.from_dict(values_dict, orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNAkJ-A7OZhb"
      },
      "outputs": [],
      "source": [
        "final_df_lst = []\n",
        "for index, row in new_dataframe.iterrows():\n",
        "    result_dataframe = pd.DataFrame()\n",
        "    target_variable = row[\"Target Variable\"]\n",
        "    pre_week = row[\"Previous Week\"]\n",
        "    curr_week = row[\"Current Week\"]\n",
        "    value1 = row[\"Previous Week Value\"]\n",
        "    value2 = row[\"Current Week Value\"]\n",
        "    tag = row[\"Tagging\"]\n",
        "    mapped_list = corr_selection[target_variable]\n",
        "    mapped_list = list(set(mapped_list))\n",
        "    previous_week_value = weekly[(weekly[\"WeekCount\"] == pre_week)][mapped_list]\n",
        "    previous_week_value = previous_week_value.values[0]\n",
        "    curr_week_values = weekly[(weekly[\"WeekCount\"] == curr_week)][mapped_list]\n",
        "    curr_week_values = curr_week_values.values[0]\n",
        "    current_row_df = pd.DataFrame([previous_week_value.tolist()], columns=mapped_list)\n",
        "    current_row_df = current_row_df.append(pd.DataFrame([curr_week_values.tolist()], columns=mapped_list), ignore_index=True)\n",
        "    current_row_df = current_row_df.T.reset_index()\n",
        "    current_row_df.columns = [\"Explainatory_Variable\", \"EV Previous Week Value\", \"EV Current Week Value\"]\n",
        "    current_row_df[\"Target Variable\"] = target_variable\n",
        "    current_row_df[\"Previous Week\"] = pre_week\n",
        "    current_row_df[\"Current Week\"] = curr_week\n",
        "    current_row_df[\"Previous Week Value\"] = value1\n",
        "    current_row_df[\"Current Week Value\"] = value2\n",
        "    current_row_df[\"Tagging\"] = tag\n",
        "    final_df_lst.append(current_row_df)\n",
        "explain_wanted = pd.concat(final_df_lst, ignore_index=True, axis=0, keys=range(len(final_df_lst)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBQJeUTVOm-d"
      },
      "source": [
        "#Unwanted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xSHCDbzOmpe"
      },
      "outputs": [],
      "source": [
        "grouped = main1_unwanted.groupby('Week pair')\n",
        "week_pair_lst = []\n",
        "best_two_lst = []\n",
        "worst_two_lst = []\n",
        "for name, group in grouped:\n",
        "  sort_week_df = group.sort_values('Values', ascending=False)\n",
        "  sort_week_df_1 = sort_week_df.reset_index().iloc[:,1:]\n",
        "  week_pair = sort_week_df_1.loc[0, 'Week pair']\n",
        "  best_two = sort_week_df_1.loc[[0, 1], 'Value'].tolist()\n",
        "  worst_two = sort_week_df_1.loc[[len(sort_week_df_1) - 1, len(sort_week_df_1) - 2], 'Value'].tolist()\n",
        "  week_pair_lst.append(week_pair)\n",
        "  best_two_lst.append(worst_two)\n",
        "  worst_two_lst.append(best_two)\n",
        "target_df = pd.DataFrame({\"Week pair\": week_pair_lst, \"Best_Two\": best_two_lst, \"Worst_Two\": worst_two_lst})\n",
        "target_df.set_index(\"Week pair\", inplace= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyS065mRNqL8"
      },
      "outputs": [],
      "source": [
        "target_df_new = pd.DataFrame(columns=['Previous Week', 'Current Week', 'Worst 1', 'Worst 2', 'Best 1', 'Best 2'])\n",
        "\n",
        "for index, row in target_df.iterrows():\n",
        "    previous_week, current_week = index.split('_')\n",
        "    worst_values = row['Worst_Two']\n",
        "    best_values = row['Best_Two']\n",
        "\n",
        "    # Check if the array is not empty before accessing its elements\n",
        "    worst_1 = worst_values[0] if len(worst_values) > 0 else None\n",
        "    worst_2 = worst_values[1] if len(worst_values) > 1 else None\n",
        "\n",
        "    best_1 = best_values[0] if len(best_values) > 0 else None\n",
        "    best_2 = best_values[1] if len(best_values) > 1 else None\n",
        "\n",
        "    new_row = {\n",
        "        'Previous Week': previous_week,\n",
        "        'Current Week': current_week,\n",
        "        'Worst 1': worst_1,\n",
        "        'Worst 2': worst_2,\n",
        "        'Best 1': best_1,\n",
        "        'Best 2': best_2\n",
        "    }\n",
        "\n",
        "    target_df_new = target_df_new.append(new_row, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya0lPdHPOxCu"
      },
      "outputs": [],
      "source": [
        "melted_df = pd.melt(target_df_new, id_vars=['Previous Week', 'Current Week'], value_vars=['Worst 1', 'Worst 2', 'Best 1', 'Best 2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGeIb8dIOxFH"
      },
      "outputs": [],
      "source": [
        "values_dict = {}\n",
        "\n",
        "for index, row in melted_df.iterrows():\n",
        "    previous_week = row['Previous Week']\n",
        "    current_week = row['Current Week']  # Assuming you have a column named 'Current Week'\n",
        "    variable = row['value']\n",
        "\n",
        "    # Use these values to get the corresponding value from the original DataFrame for the previous week\n",
        "    previous_week_value = weekly[(weekly[\"WeekCount\"] == previous_week)][[variable]] #weekly.loc[previous_week, variable]#if pd.notna(previous_week) and variable in weekly.columns else 0\n",
        "    previous_week_value = previous_week_value.values[0][0]\n",
        "\n",
        "    # Use these values to get the corresponding value from the original DataFrame for the current week\n",
        "    current_week_value = weekly[(weekly[\"WeekCount\"] == current_week)][[variable]] #current_week_value = weekly.loc[current_week, variable] #if pd.notna(current_week) and variable in weekly.columns else 0\n",
        "    current_week_value = current_week_value.values[0][0]\n",
        "\n",
        "    # Calculate the difference between Previous Week Value and Current Week Value\n",
        "    difference = current_week_value - previous_week_value\n",
        "\n",
        "    # Add the values to the dictionary with the corresponding index and column names\n",
        "    values_dict[index] = {\n",
        "        'Previous Week': previous_week,\n",
        "        'Current Week': current_week,\n",
        "        'Target Variable': variable,\n",
        "        'Previous Week Value': previous_week_value,\n",
        "        'Current Week Value': current_week_value,\n",
        "        'Difference': difference,\n",
        "        'Tagging': row['variable']  # Assuming 'variable' is the column name in melted_df\n",
        "    }\n",
        "\n",
        "# Create a new DataFrame from the dictionary\n",
        "new_dataframe = pd.DataFrame.from_dict(values_dict, orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v49gdT1eOxHH"
      },
      "outputs": [],
      "source": [
        "final_df_lst = []\n",
        "for index, row in new_dataframe.iterrows():\n",
        "    result_dataframe = pd.DataFrame()\n",
        "    target_variable = row[\"Target Variable\"]\n",
        "    pre_week = row[\"Previous Week\"]\n",
        "    curr_week = row[\"Current Week\"]\n",
        "    value1 = row[\"Previous Week Value\"]\n",
        "    value2 = row[\"Current Week Value\"]\n",
        "    tag = row[\"Tagging\"]\n",
        "    mapped_list = corr_selection[target_variable]\n",
        "    mapped_list = list(set(mapped_list))\n",
        "    previous_week_value = weekly[(weekly[\"WeekCount\"] == pre_week)][mapped_list]\n",
        "    previous_week_value = previous_week_value.values[0]\n",
        "    curr_week_values = weekly[(weekly[\"WeekCount\"] == curr_week)][mapped_list]\n",
        "    curr_week_values = curr_week_values.values[0]\n",
        "    current_row_df = pd.DataFrame([previous_week_value.tolist()], columns=mapped_list)\n",
        "    current_row_df = current_row_df.append(pd.DataFrame([curr_week_values.tolist()], columns=mapped_list), ignore_index=True)\n",
        "    current_row_df = current_row_df.T.reset_index()\n",
        "    current_row_df.columns = [\"Explainatory_Variable\", \"EV Previous Week Value\", \"EV Current Week Value\"]\n",
        "    current_row_df[\"Target Variable\"] = target_variable\n",
        "    current_row_df[\"Previous Week\"] = pre_week\n",
        "    current_row_df[\"Current Week\"] = curr_week\n",
        "    current_row_df[\"Previous Week Value\"] = value1\n",
        "    current_row_df[\"Current Week Value\"] = value2\n",
        "    current_row_df[\"Tagging\"] = tag\n",
        "    final_df_lst.append(current_row_df)\n",
        "explain_unwanted = pd.concat(final_df_lst, ignore_index=True, axis=0, keys=range(len(final_df_lst)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nowYk9M23JcL"
      },
      "outputs": [],
      "source": [
        "explain_unwanted[\"Distribution_Tag\"] = \"Unwanted\"\n",
        "explain_wanted[\"Distribution_Tag\"] = \"Wanted\"\n",
        "explain_overall = pd.concat([explain_wanted, explain_unwanted])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDsBw89WWGMk"
      },
      "outputs": [],
      "source": [
        "explain_overall_filtered = explain_overall[(explain_overall[\"EV Previous Week Value\"] != 0) & (explain_overall[\"EV Current Week Value\"] != 0) & (explain_overall[\"Previous Week Value\"] != 0) & (explain_overall[\"Current Week Value\"] != 0)]\n",
        "explain_overall_filtered[explain_overall_filtered[\"Previous Week Value\"] != explain_overall_filtered[\"Current Week Value\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hnIc4foWGOv"
      },
      "outputs": [],
      "source": [
        "explain_overall_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v6tsbuJWGSI"
      },
      "outputs": [],
      "source": [
        "summary_view = explain_overall_filtered[['Target Variable', 'Previous Week', 'Current Week', 'Previous Week Value', 'Current Week Value']]\n",
        "summary_view = summary_view.drop_duplicates()\n",
        "summary_view[\"Delta\"] = (summary_view[\"Current Week Value\"] - summary_view[\"Previous Week Value\"]) * 100 / summary_view[\"Current Week Value\"]\n",
        "summary_view[\"Delta Type\"] = \"\"\n",
        "summary_view[\"Number of Explainatory Variables\"] = 0\n",
        "for i in range (len(summary_view)):\n",
        "  target_variable = summary_view.iloc[i, 0]\n",
        "  prev_week = summary_view.iloc[i, 1]\n",
        "  curr_week = summary_view.iloc[i, 2]\n",
        "  dummy = explain_overall_filtered[(explain_overall_filtered[\"Target Variable\"] == target_variable) & (explain_overall_filtered[\"Previous Week\"] == prev_week) & (explain_overall_filtered[\"Current Week\"] == curr_week)]\n",
        "  num = len(dummy)\n",
        "  summary_view.iloc[i, 7] = num\n",
        "  if summary_view.iloc[i, 5] > 0:\n",
        "    summary_view.iloc[i, 6] = \"Good\"\n",
        "  else:\n",
        "    summary_view.iloc[i, 6] = \"Bad\"\n",
        "summary_view = summary_view.sort_values(by='Previous Week')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulpHB-aj9U3a"
      },
      "outputs": [],
      "source": [
        "summary_view[(summary_view[\"Current Week\"] == \"2023-w46\") | (summary_view[\"Current Week\"] == \"2023-w47\") | (summary_view[\"Current Week\"] == \"2023-w48\")]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "good_list = []\n",
        "bad_list = []\n",
        "\n",
        "for x in range(1, len(List)):\n",
        "  start_week = List[x-1]\n",
        "  end_week = List[x]\n",
        "  sub_explain = summary_view[(summary_view[\"Previous Week\"] == start_week) & (summary_view[\"Current Week\"] == end_week)]\n",
        "  parameter_lst = sub_explain[\"Target Variable\"].tolist()[-1:] + sub_explain[\"Target Variable\"].tolist()[0:1]\n",
        "  good = len(sub_explain[sub_explain[\"Delta Type\"] == \"Good\"])\n",
        "  bad = len(sub_explain[sub_explain[\"Delta Type\"] == \"Bad\"])\n",
        "  for parameter_idx in range (len(parameter_lst)):\n",
        "    parameter = parameter_lst[parameter_idx]\n",
        "    view_table = weekly[(weekly[\"WeekCount\"] == start_week) | (weekly[\"WeekCount\"] == end_week)]\n",
        "    view_table.set_index(\"WeekCount\", inplace=True)\n",
        "    view_table.loc[\"Difference\"] = view_table.iloc[1, :] - view_table.iloc[0, :]\n",
        "    view_table.loc[\"Difference_Percentage\"] = pd.Series()\n",
        "    cols = view_table.columns.tolist()\n",
        "    for j in range (len(cols)):\n",
        "      if view_table.iloc[1, j] != 0:\n",
        "        view_table.iloc[3, j] = (view_table.iloc[1, j] - view_table.iloc[0, j]) * 100 / view_table.iloc[1, j]\n",
        "      else:\n",
        "        view_table.iloc[3, j] = 0\n",
        "    tag = sub_explain[sub_explain[\"Target Variable\"] == parameter][[\"Delta Type\"]].values.tolist()[0][0]\n",
        "    if tag == \"Good\":\n",
        "      view_table = view_table.sort_values(by='Difference_Percentage', axis=1, ascending=False)\n",
        "    else:\n",
        "      view_table = view_table.sort_values(by='Difference_Percentage', axis=1, ascending=True)\n",
        "    cols = view_table.columns.tolist()\n",
        "    target_vars = []\n",
        "    for i in cols:\n",
        "      if i in col_tar and i != parameter:\n",
        "        target_vars.append(i)\n",
        "      if len(target_vars) == 2:\n",
        "        break\n",
        "    view_table.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    view_table = view_table.dropna(axis=1, how='any')\n",
        "    order = [parameter] + target_vars + [col for col in view_table.columns if col != parameter and col != \"WeekCount\"]\n",
        "    view_table = view_table[order]\n",
        "    view_table = view_table.iloc[:, list(range(11)) + [-3, -2, -1]]\n",
        "    view_table = round(view_table, 2)\n",
        "    lst = []\n",
        "    lst.append(start_week)\n",
        "    lst.append(end_week)\n",
        "    lst.append(view_table.columns.tolist()[0])\n",
        "    lst.append(view_table.iloc[3, 0])\n",
        "    lst.append(view_table.iloc[1, 0])\n",
        "    lst.append(view_table.iloc[2, 0])\n",
        "    lst.append(view_table.columns.tolist()[1])\n",
        "    lst.append(view_table.iloc[1, 0])\n",
        "    lst.append(view_table.iloc[2, 0])\n",
        "    lst.append(view_table.columns.tolist()[2])\n",
        "    lst.append(view_table.iloc[1, 0])\n",
        "    lst.append(view_table.iloc[2, 0])\n",
        "    lst.append(view_table.columns.tolist()[3])\n",
        "    lst.append(view_table.iloc[1, 0])\n",
        "    lst.append(view_table.iloc[2, 0])\n",
        "    if parameter_idx == 0:\n",
        "      good_list.append(lst)\n",
        "    else:\n",
        "      bad_list.append(lst)"
      ],
      "metadata": {
        "id": "a1wmIFmC6Gtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzUmwoBacXjr"
      },
      "outputs": [],
      "source": [
        "start_week = \"2023-w46\"\n",
        "end_week = \"2023-w47\"\n",
        "sub_explain = summary_view[(summary_view[\"Previous Week\"] == start_week) & (summary_view[\"Current Week\"] == end_week)]\n",
        "print(f\"{len(sub_explain)} Key Variables are: \", \" ,\".join(sub_explain[\"Target Variable\"].tolist()))\n",
        "good = len(sub_explain[sub_explain[\"Delta Type\"] == \"Good\"])\n",
        "bad = len(sub_explain[sub_explain[\"Delta Type\"] == \"Bad\"])\n",
        "print(f\"{good} of the Key Variables showed Positive Variation\")\n",
        "print(f\"{bad} of the Key Variables showed Negative Variation\")\n",
        "parameter = input(\"Parameter to View: \")\n",
        "view_table = weekly[(weekly[\"WeekCount\"] == start_week) | (weekly[\"WeekCount\"] == end_week)]\n",
        "view_table.set_index(\"WeekCount\", inplace=True)\n",
        "view_table.loc[\"Difference\"] = view_table.iloc[1, :] - view_table.iloc[0, :]\n",
        "view_table.loc[\"Difference_Percentage\"] = pd.Series()\n",
        "cols = view_table.columns.tolist()\n",
        "for j in range (len(cols)):\n",
        "  if view_table.iloc[1, j] != 0:\n",
        "    view_table.iloc[3, j] = (view_table.iloc[1, j] - view_table.iloc[0, j]) * 100 / view_table.iloc[1, j]\n",
        "  else:\n",
        "    view_table.iloc[3, j] = 0\n",
        "tag = sub_explain[sub_explain[\"Target Variable\"] == parameter][[\"Delta Type\"]].values.tolist()[0][0]\n",
        "if tag == \"Good\":\n",
        "  view_table = view_table.sort_values(by='Difference_Percentage', axis=1, ascending=False)\n",
        "else:\n",
        "  view_table = view_table.sort_values(by='Difference_Percentage', axis=1, ascending=True)\n",
        "cols = view_table.columns.tolist()\n",
        "target_vars = []\n",
        "for i in cols:\n",
        "  if i in col_tar and i != parameter:\n",
        "    target_vars.append(i)\n",
        "  if len(target_vars) == 2:\n",
        "    break\n",
        "view_table.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "view_table = view_table.dropna(axis=1, how='any')\n",
        "order = [parameter] + target_vars + [col for col in view_table.columns if col != parameter and col != \"WeekCount\"]\n",
        "view_table = view_table[order]\n",
        "view_table = view_table.iloc[:, list(range(11)) + [-3, -2, -1]]\n",
        "round(view_table, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fGOnfcxxe8J"
      },
      "outputs": [],
      "source": [
        "df = summary_view.copy()\n",
        "good_occurrences = df[df['Delta Type'] == 'Good']\n",
        "bad_occurrences = df[df['Delta Type'] == 'Bad']\n",
        "\n",
        "good_count = good_occurrences.shape[0]\n",
        "bad_count = bad_occurrences.shape[0]\n",
        "\n",
        "good_info = good_occurrences[['Current Week', 'Target Variable']]\n",
        "\n",
        "bad_info = bad_occurrences[['Current Week', 'Target Variable']]\n",
        "\n",
        "good_grouped = good_info.groupby('Current Week').agg({'Target Variable': ['count', list]})\n",
        "good_grouped.columns = ['Good Variation Count', 'Good Variation Target Variable List']\n",
        "\n",
        "bad_grouped = bad_info.groupby('Current Week').agg({'Target Variable': ['count', list]})\n",
        "bad_grouped.columns = ['Bad Variation Count', 'Bad Variation Target Variable List']\n",
        "\n",
        "result_df = pd.merge(good_grouped, bad_grouped, how='outer', on='Current Week').fillna(0).reset_index()\n",
        "result_df.sort_values(by='Current Week', ascending = True)\n",
        "result_df[\"Overall\"] = result_df[\"Good Variation Count\"] + result_df[\"Bad Variation Count\"]\n",
        "result_df[\"Good Variation Percentage\"] = result_df[\"Good Variation Count\"] * 100 / result_df[\"Overall\"]\n",
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fn95AfLzzgd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 6))\n",
        "plt.plot(result_df['Current Week'], result_df['Good Variation Count'], marker='o', linestyle='-', color='green', label='Good')\n",
        "plt.plot(result_df['Current Week'], result_df['Bad Variation Count'], marker='o', linestyle='-', color='red', label='Bad')\n",
        "\n",
        "plt.xlabel('Current Week')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Good and Bad Variation Counts Over Weeks')\n",
        "plt.xticks(rotation = 90)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsYuRKG4xewq"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 6))\n",
        "plt.plot(result_df['Current Week'], result_df['Good Variation Count'], marker='o', linestyle='-', color='green', label='Good')\n",
        "# plt.plot(result_df['Current Week'], result_df['Bad Variation Count'], marker='o', linestyle='-', color='red', label='Bad')\n",
        "\n",
        "plt.xlabel('Current Week')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Good Counts Over Weeks')\n",
        "plt.xticks(rotation = 90)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eQRRPAYzpa4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 6))\n",
        "# plt.plot(result_df['Current Week'], result_df['Good Variation Count'], marker='o', linestyle='-', color='green', label='Good')\n",
        "plt.plot(result_df['Current Week'], result_df['Bad Variation Count'], marker='o', linestyle='-', color='red', label='Bad')\n",
        "\n",
        "plt.xlabel('Current Week')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Bad Variation Counts Over Weeks')\n",
        "plt.xticks(rotation = 90)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvTCj0n-0v3g"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 6))\n",
        "plt.plot(result_df['Current Week'], result_df['Good Variation Percentage'], marker='o', linestyle='-', color='green', label='Good')\n",
        "# plt.plot(result_df['Current Week'], result_df['Bad Variation Count'], marker='o', linestyle='-', color='red', label='Bad')\n",
        "\n",
        "plt.xlabel('Current Week')\n",
        "plt.ylabel('Percentage')\n",
        "plt.title('Good and Bad Variation Counts Over Weeks')\n",
        "plt.xticks(rotation = 90)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n6wAvt7PwBO"
      },
      "outputs": [],
      "source": [
        "# import gspread\n",
        "# from oauth2client.service_account import ServiceAccountCredentials\n",
        "# from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
        "# # define the scope\n",
        "# scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
        "# creds = ServiceAccountCredentials.from_json_keyfile_dict({\n",
        "#   \"type\": \"service_account\",\n",
        "#   \"project_id\": \"healthy-keyword-407310\",\n",
        "#   \"private_key_id\": \"1c4d8d01ef23a28976fa16ec680c9a4249eb6345\",\n",
        "#   \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCzozMmgBbJJGOk\\nnM1rVoDhHQKX010JcwETSd6ustE5LAbAwmEbefAOASyAOIA24+AuxUWsIwkXhtz0\\nIvab/Xw8RdU4eHbd3PyxDTzgQZKnAOI6qC0LK5srugbU+Vbryy6x1cbFnIYRNPee\\nezDLfDkDkh0vxDvF/ZUYDt4MIqHrPw6R/1q1E+aVWslX7Xxdb8WPCrwUTaL4tJ5m\\nxD1q7T03rTQlcCXA9h9PP+qZC8m6ZS8eVvq6KeikVp9e4hJzCRU/hTeDQ8JnFtep\\nVbOnM+E0kYvguX4xDPcKt/j/VUC+qt0pG0Jr+HTMP42dh9c00v9b1ltCTiWhr3uU\\nYscTgZNzAgMBAAECggEAKw6gmYdVLtQqkA/HiwVacdrOvs56O+QVa2am2e/hh1ce\\nrUBKH1hcajbs0DT6wm1fVEaTmx1wtfNs7ZWdz6j0CMxiPZ1ePHKWHOgD4erFuILV\\nNuMNnOJQWKxHq++x3mW+pQCNGtuYJPKxsYpsmTGGxuzUUxkxjJTqEjCqF3WLowN7\\nfEELJk0h4eETo7WKTO3+Jy3XC7Wi44ezEYnQUkCs4VK0B+28ECub3xskAwTJYyC9\\nsdMtyEtiyFD5vFiV11xWoLP3WzAQCsmG/Z+Ld/OFVcgzrDGmhmrUmUTAdw2o9wnH\\ngKIjALVQwRqZcwrwm31ZLmmf2IYrWzdVpvO167mJKQKBgQD9ryEQabiyb7wBLPFr\\nP6dCi1awn2SNdUI0NPaiblSWuvVnFJjv/6RqgGrKb5/SRS+x9t4Dj/RZYNz8BVaH\\n7/xiNmzWB0YqvS/je5O2cFX7PG9/DOqrFK6vC8LMKqAmDu0rsYfneKdMEsLD08cf\\nYtCCULIqF0Q5OyE+KN/IPw+kCQKBgQC1RwU/4hcFyDT/lLU5Wpk5lgAyUNvXWKAX\\n7IK3Gx7WQOHsO0HXPiaTaX1wLVHq5PJVC+JIUuyfHf8HBkuUrpu6uAZUvNZal+zw\\nO4DuLB6AtBfCZJewSpazbG0uh2i+qaqZygLhhlOF0mySasKKdYRv7avswFOL8v/Q\\nsthzSeOymwKBgFSqy50/u74KHrRBvL1vvDIwcRl3yUaRGSx1dM9XhXHdKiRiMnz/\\nVG+bNokj4jJ6luWNoCGOPR5LftW6LlwmU5Sv13lDaj6/b6k7yFv+eCPm0suTkwrQ\\nV1Di2vvrShIJr0yC7vnpQeTY6hoTp4SpJTApIJ19sG5tdrXZTH731Q+JAoGAE2l5\\njG+/m2TnF/9qsKudAOepOboi8JqS+jzfepAy6yqjhtKJqG0FYH/JaPvRa9/8e1L8\\ndlY69IcXyj7IVTGlh4cxVl2qbqUaQFd+5QJgedPPP4faHoy+OpR/1J23f8NXe4SA\\nbnod85Sm/77i+kS/W+UNphqqV7A76LNeLmpZMcUCgYBwj6egYHM/8DwPceP8Lk9W\\ndk2nIXeNDVNZzD3fYfTThgxZF4OPj7/8DeKxr8VkWbFZS+Gu85V3rve8TmwrjD4V\\nlfsZzH5qWTdSrVcni/J7hcRide5YwtoUtICjXZnjTcWvB1SB3sXE5ca10LHVwm4n\\nTqyZA1Et+lqb8z5lzeckCg==\\n-----END PRIVATE KEY-----\\n\",\n",
        "#   \"client_email\": \"colab-speadsheet@healthy-keyword-407310.iam.gserviceaccount.com\",\n",
        "#   \"client_id\": \"108869888846298899259\",\n",
        "#   \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "#   \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "#   \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "#   \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/colab-speadsheet%40healthy-keyword-407310.iam.gserviceaccount.com\",\n",
        "#   \"universe_domain\": \"googleapis.com\"\n",
        "# }, scope)\n",
        "# # add credentials to the account\n",
        "# client = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sheet = client.create('ToffeeObserveSequential')\n",
        "# sheet.share('rahul.soni@superu.ai', perm_type='user', role='writer')#"
      ],
      "metadata": {
        "id": "KmqKmmLgEd_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# explain_overall_filtered.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "# explain_overall_filtered.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "4XlKv8U8FD2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ws1=sheet.add_worksheet('Explain table', rows=\"31000\", cols=\"11\")\n",
        "# ws1 = sheet.worksheet(\"Explain table\")\n",
        "# set_with_dataframe(ws1,explain_overall_filtered )"
      ],
      "metadata": {
        "id": "ppeFJcGoEkPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ws1=sheet.add_worksheet('Summary Table', rows=\"900\", cols=\"10\")\n",
        "# ws1 = sheet.worksheet(\"Summary Table\")\n",
        "# set_with_dataframe(ws1,summary_view )"
      ],
      "metadata": {
        "id": "9tNw7boMEmwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spreadsheet = client.open('SoftTouchObserve')\n",
        "# sheet_title_to_delete = \"Sheet1\"\n",
        "# worksheets = spreadsheet.worksheets()\n",
        "# worksheet_to_delete = None\n",
        "# for worksheet in worksheets:\n",
        "#     if worksheet.title == sheet_title_to_delete:\n",
        "#         worksheet_to_delete = worksheet\n",
        "#         break\n",
        "# if worksheet_to_delete:\n",
        "#     spreadsheet.del_worksheet(worksheet_to_delete)\n",
        "#     print(f\"Sheet deleted.\")\n",
        "# else:\n",
        "#     print(f\"Sheet not found.\")"
      ],
      "metadata": {
        "id": "UQ591znSEvKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FnAsLqhyEzwz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}